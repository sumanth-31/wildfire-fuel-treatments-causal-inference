{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df2276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2015] LC15_Dist_200.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from LC15_Dist_200.tif.vat.dbf\n",
      "[2016] LC16_Dist_200.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from LC16_Dist_200.tif.vat.dbf\n",
      "[2017] LC17_Dist_220.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from LC17_Dist_220.tif.vat.dbf\n",
      "[2018] LC18_Dist_220.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from LC18_Dist_220.tif.vat.dbf\n",
      "[2019] LC19_Dist_220.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from LC19_Dist_220.tif.vat.dbf\n",
      "[2020] LC20_Dist_220.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from LC20_Dist_220.tif.vat.dbf\n",
      "[2021] LC21_Dist_230.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from LC21_Dist_230.tif.vat.dbf\n",
      "[2022] LC22_Dist_230.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from LC22_Dist_230.tif.vat.dbf\n",
      "[2023] LC23_Dist_240.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from LC23_Dist_240.tif.vat.dbf\n",
      "[2024] LC24_Dist_250.tif: thinning codes = [2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437] from LC24_Dist_250.tif.vat.dbf\n",
      "[1999] us_dist1999.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from us_dist1999.tif.vat.dbf\n",
      "[2000] us_dist2000.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from us_dist2000.tif.vat.dbf\n",
      "[2001] us_dist2001.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from us_dist2001.tif.vat.dbf\n",
      "[2002] us_dist2002.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from us_dist2002.tif.vat.dbf\n",
      "[2003] us_dist2003.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from us_dist2003.tif.vat.dbf\n",
      "[2004] us_dist2004.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from us_dist2004.tif.vat.dbf\n",
      "[2005] us_dist2005.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from us_dist2005.tif.vat.dbf\n",
      "[2006] us_dist2006.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from us_dist2006.tif.vat.dbf\n",
      "[2007] us_dist2007.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from us_dist2007.tif.vat.dbf\n",
      "[2008] us_dist2008.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from us_dist2008.tif.vat.dbf\n",
      "[2009] us_dist2009.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from us_dist2009.tif.vat.dbf\n",
      "[2010] us_dist2010.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from us_dist2010.tif.vat.dbf\n",
      "[2011] us_dist2011.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from us_dist2011.tif.vat.dbf\n",
      "[2012] us_dist2012.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from us_dist2012.tif.vat.dbf\n",
      "[2013] us_dist2013.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from us_dist2013.tif.vat.dbf\n",
      "[2014] us_dist2014.tif: thinning codes = [441, 442, 443, 741, 742, 743, 941] from us_dist2014.tif.vat.dbf\n"
     ]
    }
   ],
   "source": [
    "# build_thinning_events.py\n",
    "# Step 1: scan disturbance rasters, keep only \"Thinning\" pixels, write partitioned Parquet by year/tile.\n",
    "\n",
    "import os, re, uuid, pathlib, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from dbfread import DBF\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# ---------------- config ----------------\n",
    "ROOT = Path(\"../../Dataset\")\n",
    "DATASET = ROOT / \"AnnualDisturbance_1999_present\"\n",
    "OUT = Path(\"../../Outputs/treatments\")\n",
    "TILE = 4096  # tile size in pixels\n",
    "BAND = 1     # first band\n",
    "TREATMENT_NAME = \"Thinning\"\n",
    "# ----------------------------------------\n",
    "\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def find_tifs(base: Path):\n",
    "    # handles both US_DISTYYYY and LF20YY_Dist_*** folders\n",
    "    return sorted(base.rglob(\"Tif/*.tif\"))\n",
    "\n",
    "def year_from_path(p: Path):\n",
    "    m = re.search(r\"(?:US_DIST|LF)\\D*?(\\d{4})\", str(p))\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    # fallback: try to read raster tag\n",
    "    try:\n",
    "        with rasterio.open(p) as src:\n",
    "            y = src.tags().get(\"DIST_YEAR\") or src.tags().get(\"Year\")\n",
    "            return int(y) if y else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def find_vat_for_tif(tif_path: Path):\n",
    "    # typical: <name>.tif.vat.dbf in same folder\n",
    "    cand = tif_path.parent.glob(f\"{tif_path.name}.vat.dbf\")\n",
    "    for c in cand:\n",
    "        return c\n",
    "    # also scan any *.vat.dbf in folder\n",
    "    anyvat = list(tif_path.parent.glob(\"*.vat.dbf\"))\n",
    "    if anyvat:\n",
    "        return anyvat[0]\n",
    "    # fallback: look in sibling CSV_Data for a codes CSV\n",
    "    csv_dir = tif_path.parents[1] / \"CSV_Data\"\n",
    "    if csv_dir.exists():\n",
    "        csvs = sorted(csv_dir.glob(\"*.csv\"))\n",
    "        if csvs:\n",
    "            return csvs[0]\n",
    "    return None\n",
    "\n",
    "def load_code_table(vat_path: Path):\n",
    "    # returns DataFrame with columns: CODE(int), DIST_TYPE(str)\n",
    "    if vat_path.suffix.lower() == \".dbf\":\n",
    "        df = pd.DataFrame(DBF(str(vat_path), load=True))\n",
    "    elif vat_path.suffix.lower() == \".csv\":\n",
    "        df = pd.read_csv(vat_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Unsupported VAT/CSV: {vat_path}\")\n",
    "\n",
    "    # normalize headers\n",
    "    df.columns = [c.strip().upper() for c in df.columns]\n",
    "    # code column candidates\n",
    "    for k in (\"VALUE\", \"GRIDCODE\", \"CODE\", \"VALUE_\"):\n",
    "        if k in df.columns:\n",
    "            code_col = k\n",
    "            break\n",
    "    else:\n",
    "        raise KeyError(f\"No code column in {vat_path}\")\n",
    "    # type column candidates across eras\n",
    "    for tcol in (\"DIST_TYPE\", \"DISTTYPE\", \"DIST_TYPE_\", \"DIST_TYPE1\", \"DIST_TYPE2\", \"DIST_TYPE3\", \"DIST_TYPE4\", \"DISTTYPE1\", \"DISTTYPE2\", \"DIST_TYPE_\", \"DIST_TYPE_1\", \"DIST_TYPE_2\", \"DIST_TYPE_3\", \"DIST_TYPE_4\", \"DIST_TYPE5\", \"DIST_TYPE6\", \"DIST_TYPE7\", \"DIST_TYPE8\", \"DIST_TYPE9\", \"DISTTYPE_\",\n",
    "                 \"DIST_TYPE \", \"DISTTYPE \", \"DIST TYPE\", \"DIST TYPE \", \"DIST TYPE_\",\n",
    "                 \"DIST_TYPE__1\", \"DIST_TYPE__2\", \"DIST_TYPE__3\", \"DIST_TYPE__4\", \"DIST_TYPE__5\"):\n",
    "        if tcol in df.columns:\n",
    "            type_col = tcol\n",
    "            break\n",
    "    # older 1999â€“2008 metadata uses Dist_Type (case-insens)\n",
    "    if \"DIST_TYPE\" not in df.columns and \"DISTTYPE\" not in df.columns:\n",
    "        for alt in (\"DIST_TYPE\", \"DISTTYPE\", \"DIST TYPE\"):\n",
    "            if alt in df.columns:\n",
    "                type_col = alt\n",
    "                break\n",
    "    if 'DIST_TYPE' not in df.columns and 'DISTTYPE' not in df.columns:\n",
    "        # try Dist_Type case-sensitive fallback\n",
    "        if \"DIST_TYPE\" not in df.columns and \"DISTTYPE\" not in df.columns:\n",
    "            # already tried uppercase; check original case on a copy\n",
    "            pass\n",
    "    # Final robust pick\n",
    "    if 'DIST_TYPE' in df.columns:\n",
    "        type_col = 'DIST_TYPE'\n",
    "    elif 'DISTTYPE' in df.columns:\n",
    "        type_col = 'DISTTYPE'\n",
    "    elif 'DIST TYPE' in df.columns:\n",
    "        type_col = 'DIST TYPE'\n",
    "    elif 'DIST_TYPE1' in df.columns:\n",
    "        type_col = 'DIST_TYPE1'\n",
    "    else:\n",
    "        # last resort: look for 'DIST' and 'TYPE' in any col\n",
    "        cand = [c for c in df.columns if \"TYPE\" in c and \"DIST\" in c]\n",
    "        if cand:\n",
    "            type_col = cand[0]\n",
    "        else:\n",
    "            # older files: Dist_Type in mixed case\n",
    "            for c in df.columns:\n",
    "                if c.replace(\"_\",\"\").replace(\" \",\"\") in (\"DISTTYPE\",\"DISTURBANCETYPE\"):\n",
    "                    type_col = c\n",
    "                    break\n",
    "            else:\n",
    "                raise KeyError(f\"No disturbance type column in {vat_path}\")\n",
    "\n",
    "    out = df[[code_col, type_col]].copy()\n",
    "    out.columns = [\"CODE\", \"DIST_TYPE\"]\n",
    "    # clean\n",
    "    out[\"CODE\"] = pd.to_numeric(out[\"CODE\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    out[\"DIST_TYPE\"] = out[\"DIST_TYPE\"].astype(str).str.strip().str.lower()\n",
    "    return out.dropna(subset=[\"CODE\"])\n",
    "\n",
    "def thinning_codes(vat_df: pd.DataFrame, treat_name=TREATMENT_NAME):\n",
    "    treat = treat_name.strip().lower()\n",
    "    codes = vat_df.loc[vat_df[\"DIST_TYPE\"] == treat, \"CODE\"].dropna().astype(int).unique().tolist()\n",
    "    return set(codes)\n",
    "\n",
    "def ensure_part_dir(root: Path, year: int, ti: int, tj: int) -> Path:\n",
    "    d = root / f\"yr={year}\" / f\"tile_i={ti}\" / f\"tile_j={tj}\"\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "def write_part(df_chunk: pd.DataFrame, year: int, out_root: Path):\n",
    "    # group by tile to create partitioned files\n",
    "    for (ti, tj), g in df_chunk.groupby([\"tile_i\", \"tile_j\"]):\n",
    "        part_dir = ensure_part_dir(out_root, year, int(ti), int(tj))\n",
    "        fn = part_dir / f\"part-{uuid.uuid4().hex[:8]}.parquet\"\n",
    "        table = pa.Table.from_pandas(g, preserve_index=False)\n",
    "        pq.write_table(table, fn, compression=\"zstd\", use_dictionary=True)\n",
    "\n",
    "def process_one_tif(tif_path: Path, out_root: Path, tile_size=TILE):\n",
    "    year = year_from_path(tif_path)\n",
    "    if not year:\n",
    "        raise ValueError(f\"Cannot infer year from path: {tif_path}\")\n",
    "\n",
    "    vat_path = find_vat_for_tif(tif_path)\n",
    "    if not vat_path:\n",
    "        raise FileNotFoundError(f\"No VAT/CSV found for {tif_path}\")\n",
    "\n",
    "    vat_df = load_code_table(vat_path)\n",
    "    codes = thinning_codes(vat_df)\n",
    "    if not codes:\n",
    "        print(f\"[{year}] No '{TREATMENT_NAME}' codes in {vat_path}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    code_arr = np.array(sorted(list(codes)))\n",
    "    print(f\"[{year}] {tif_path.name}: thinning codes = {sorted(list(codes))} from {vat_path.name}\")\n",
    "\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        nodata = src.nodata\n",
    "        H, W = src.height, src.width\n",
    "        dtype = src.dtypes[0]\n",
    "\n",
    "        # stream over native blocks\n",
    "        for _, w in src.block_windows(BAND):\n",
    "            a = src.read(BAND, window=w)  # ndarray\n",
    "            if nodata is not None:\n",
    "                mask = a != nodata\n",
    "            else:\n",
    "                mask = np.ones_like(a, dtype=bool)\n",
    "\n",
    "            if not mask.any():\n",
    "                continue\n",
    "\n",
    "            # thinning pixels in this window\n",
    "            is_thin = mask & np.isin(a, code_arr)\n",
    "            if not is_thin.any():\n",
    "                continue\n",
    "\n",
    "            rr, cc = np.nonzero(is_thin)\n",
    "            rows = rr + int(w.row_off)\n",
    "            cols = cc + int(w.col_off)\n",
    "\n",
    "            tile_i = rows // tile_size\n",
    "            tile_j = cols // tile_size\n",
    "            r_in = rows % tile_size\n",
    "            c_in = cols % tile_size\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                \"year\": np.full(rows.shape[0], year, dtype=np.int16),\n",
    "                \"tile_i\": tile_i.astype(np.int32),\n",
    "                \"tile_j\": tile_j.astype(np.int32),\n",
    "                \"row\": rows.astype(np.int32),\n",
    "                \"col\": cols.astype(np.int32),\n",
    "                \"r\": r_in.astype(np.int16),\n",
    "                \"c\": c_in.astype(np.int16),\n",
    "                \"type_code\": np.full(rows.shape[0], 1, dtype=np.int16),  # 1 = Thinning\n",
    "            })\n",
    "            write_part(df, year, out_root)\n",
    "\n",
    "def main():\n",
    "    tifs = find_tifs(DATASET)\n",
    "    if not tifs:\n",
    "        raise SystemExit(f\"No GeoTIFFs under {DATASET}\")\n",
    "    for tif in tifs:\n",
    "        try:\n",
    "            process_one_tif(tif, OUT, TILE)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Failed {tif}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e088358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
