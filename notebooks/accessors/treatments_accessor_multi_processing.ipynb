{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df2276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "from pathlib import Path\n",
    "import re, uuid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from dbfread import DBF\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Can't initialize NVML\")\n",
    "\n",
    "\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "# ---- config ----s\n",
    "DATASET_ROOT = Path(\"../../Dataset\")\n",
    "DISTURBANCES_DATASET = DATASET_ROOT / \"AnnualDisturbance_1999_present\"\n",
    "TREATMENTS_OUTPUT = Path(\"../../Outputs/treatments\")\n",
    "TILE = 4096\n",
    "BAND = 1\n",
    "TREATMENT_NAME = \"Thinning\"\n",
    "WORKERS = min(12, os.cpu_count() - 4)\n",
    "FLUSH_ROWS = 200_000\n",
    "GPU_MIN_ELEMS = 2_000_000\n",
    "TREATMENTS_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def find_tifs(base: Path):\n",
    "    return sorted(base.rglob(\"Tif/*.tif\"))\n",
    "\n",
    "def year_from_path(path: Path):\n",
    "    year_regex_match = re.search(r\"(?:US_DIST|LF)\\D*?(\\d{4})\", str(path))\n",
    "    if year_regex_match: return int(year_regex_match.group(1))\n",
    "    try:\n",
    "        with rasterio.open(path) as src:\n",
    "            y = src.tags().get(\"DIST_YEAR\") or src.tags().get(\"Year\")\n",
    "            return int(y) if y else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def find_vat_for_tif(tif_path: Path):\n",
    "    for c in tif_path.parent.glob(f\"{tif_path.name}.vat.dbf\"): return c\n",
    "    anyvat = list(tif_path.parent.glob(\"*.vat.dbf\"))\n",
    "    if anyvat: return anyvat[0]\n",
    "    csv_dir = tif_path.parents[1] / \"CSV_Data\"\n",
    "    if csv_dir.exists():\n",
    "        csvs = sorted(csv_dir.glob(\"*.csv\"))\n",
    "        if csvs: return csvs[0]\n",
    "    return None\n",
    "\n",
    "def load_code_table(vat_path: Path):\n",
    "    if vat_path.suffix.lower() == \".dbf\":\n",
    "        df = pd.DataFrame(DBF(str(vat_path), load=True))\n",
    "    elif vat_path.suffix.lower() == \".csv\":\n",
    "        df = pd.read_csv(vat_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(vat_path)\n",
    "\n",
    "    df.columns = [c.strip().upper() for c in df.columns]\n",
    "\n",
    "    for k in (\"VALUE\",\"GRIDCODE\",\"CODE\",\"VALUE_\"):\n",
    "        if k in df.columns:\n",
    "            code_col = k; break\n",
    "    else:\n",
    "        raise KeyError(f\"No code col in {vat_path}\")\n",
    "\n",
    "    for tcol in (\"DIST_TYPE\",\"DISTTYPE\",\"DIST TYPE\",\"DIST_TYPE1\",\"DISTTYPE1\",\n",
    "                 \"DIST_TYPE2\",\"DISTTYPE2\",\"DIST_TYPE3\",\"DIST_TYPE4\",\n",
    "                 \"DIST_TYPE_\",\"DISTTYPE_\"):\n",
    "        if tcol in df.columns:\n",
    "            type_col = tcol; break\n",
    "    else:\n",
    "        cand = [c for c in df.columns if \"TYPE\" in c and \"DIST\" in c]\n",
    "        if not cand: raise KeyError(f\"No type col in {vat_path}\")\n",
    "        type_col = cand[0]\n",
    "\n",
    "    out = df[[code_col, type_col]].copy()\n",
    "    out.columns = [\"CODE\",\"DIST_TYPE\"]\n",
    "    out[\"CODE\"] = pd.to_numeric(out[\"CODE\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    out[\"DIST_TYPE\"] = out[\"DIST_TYPE\"].astype(str).str.strip().str.lower()  # <-- fixed\n",
    "    return out.dropna(subset=[\"CODE\"])\n",
    "\n",
    "\n",
    "def thinning_codes(vat_df: pd.DataFrame, treat_name=TREATMENT_NAME):\n",
    "    t=treat_name.strip().lower()\n",
    "    return set(vat_df.loc[vat_df[\"DIST_TYPE\"]==t,\"CODE\"].dropna().astype(int).unique().tolist())\n",
    "\n",
    "def ensure_part_dir(root: Path, year: int, ti: int, tj: int)->Path:\n",
    "    d = root / f\"yr={year}\" / f\"tile_i={ti}\" / f\"tile_j={tj}\"\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "def write_part_fast(df: pd.DataFrame, out_dir: Path):\n",
    "    fn = out_dir / f\"part-{uuid.uuid4().hex[:8]}.parquet\"\n",
    "    pq.write_table(pa.Table.from_pandas(df, preserve_index=False),\n",
    "                   fn, compression=\"zstd\", use_dictionary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e088358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CPU-only worker ----\n",
    "def process_one_tif_cpu(tif_path: str, out_root: str, tile_size: int, gpu_min_elems: int=0):\n",
    "    from pathlib import Path\n",
    "    import numpy as np, pandas as pd, rasterio, uuid\n",
    "    tif_path = Path(tif_path); out_root = Path(out_root)\n",
    "\n",
    "    year = year_from_path(tif_path)\n",
    "    if not year: return (tif_path.name, 0, 0)\n",
    "    vat_path = find_vat_for_tif(tif_path)\n",
    "    if not vat_path: return (tif_path.name, 0, 0)\n",
    "\n",
    "    vat_df = load_code_table(vat_path)\n",
    "    codes = thinning_codes(vat_df)\n",
    "    if not codes: return (tif_path.name, 0, 0)\n",
    "\n",
    "    code_arr = np.array(sorted(list(codes)))\n",
    "    buffers, kept, written = {}, 0, 0\n",
    "\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        nodata = src.nodata\n",
    "        for _, w in src.block_windows(1):\n",
    "            a = src.read(1, window=w)\n",
    "            m = (a != nodata) if nodata is not None else np.ones_like(a, bool)\n",
    "            if not m.any(): continue\n",
    "            sel = m & np.isin(a, code_arr)\n",
    "            if not sel.any(): continue\n",
    "\n",
    "            rr, cc = np.nonzero(sel)\n",
    "            rows, cols = rr + int(w.row_off), cc + int(w.col_off)\n",
    "            kept += rows.size\n",
    "\n",
    "            tile_i = rows // tile_size;  tile_j = cols // tile_size\n",
    "            r_in = rows % tile_size;     c_in = cols % tile_size\n",
    "\n",
    "            df = pd.DataFrame({\n",
    "                \"year\": np.full(rows.shape[0], year, dtype=np.int16),\n",
    "                \"tile_i\": tile_i.astype(np.int32),\n",
    "                \"tile_j\": tile_j.astype(np.int32),\n",
    "                \"row\": rows.astype(np.int32),\n",
    "                \"col\": cols.astype(np.int32),\n",
    "                \"r\": r_in.astype(np.int16),\n",
    "                \"c\": c_in.astype(np.int16),\n",
    "                \"type_code\": np.full(rows.shape[0], 1, dtype=np.int16),\n",
    "            })\n",
    "\n",
    "            for (ti, tj), g in df.groupby([\"tile_i\",\"tile_j\"], sort=False):\n",
    "                key = (int(ti), int(tj))\n",
    "                lst = buffers.get(key, [])\n",
    "                lst.append(g); buffers[key] = lst\n",
    "                if sum(x.shape[0] for x in lst) >= FLUSH_ROWS:\n",
    "                    out_dir = ensure_part_dir(Path(out_root), year, key[0], key[1])\n",
    "                    write_part_fast(pd.concat(lst, ignore_index=True), out_dir)\n",
    "                    buffers[key] = []; written += 1\n",
    "\n",
    "    for (ti, tj), lst in buffers.items():\n",
    "        if lst:\n",
    "            out_dir = ensure_part_dir(Path(out_root), year, int(ti), int(tj))\n",
    "            write_part_fast(pd.concat(lst, ignore_index=True), out_dir)\n",
    "            written += 1\n",
    "\n",
    "    return (tif_path.name, kept, written)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa79d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files=26  workers=12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 26\n",
      "kept_pixels: 40490314\n",
      "parquet_parts: 4355\n"
     ]
    }
   ],
   "source": [
    "# ---- Driver using fork (works in notebooks) ----\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing as mp, os\n",
    "\n",
    "# do NOT import torch anywhere in this notebook now\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\",\"1\"); os.environ.setdefault(\"MKL_NUM_THREADS\",\"1\")\n",
    "\n",
    "ctx = mp.get_context(\"fork\")  # safe with rasterio, no CUDA needed\n",
    "tifs = find_tifs(DISTURBANCES_DATASET)\n",
    "print(f\"files={len(tifs)}  workers={WORKERS}\")\n",
    "\n",
    "stats = []\n",
    "with ProcessPoolExecutor(max_workers=WORKERS, mp_context=ctx) as ex:\n",
    "    futs = {ex.submit(process_one_tif_cpu, str(t), str(TREATMENTS_OUTPUT), TILE): t for t in tifs}\n",
    "    for f in as_completed(futs):\n",
    "        try:\n",
    "            stats.append(f.result())\n",
    "        except Exception as e:\n",
    "            print(\"fail:\", futs[f], e)\n",
    "\n",
    "print(\"processed:\", len(stats))\n",
    "print(\"kept_pixels:\", sum(k for _, k, _ in stats))\n",
    "print(\"parquet_parts:\", sum(w for _, _, w in stats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25819f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
